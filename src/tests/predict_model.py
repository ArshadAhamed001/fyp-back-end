# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SpkaE-O9cY59GqTxYwbWAFrzSVuxpyDl
"""

import pandas as pd 
import numpy as np
import os



CLASSES, gems = [], [] # names of classes, count of images for each class

for root, dirs, files in os.walk('/content/drive/MyDrive/FYP Implementation/DataSet-01'):
    f = os.path.basename(root)    # get class name - Amethyst, Onyx, etc    
        
    if len(files) > 0:
        gems.append(len(files))
        if f not in CLASSES:
            CLASSES.append(f) # add folder name
    
    # uncomment this block if you want a text output about each subfolder
#     count_dirs = 0
#     for f in dirs:           # count subfolders
#         count_dirs += 1
#     depth = root.split(os.sep)
#     print((len(depth) - 2) * '--'+'>', '{}:\t {} folders, {} imgs'.format(os.path.basename(root), count_dirs, gems[-1] if gems!=[] else 0)) 
    
gems_count = len(CLASSES) # 87 = number of classes
print('{} classes with {} images in total'.format(len(CLASSES), sum(gems)))

f, ax = plt.subplots(figsize=(15,6))
if(gems[0])<10:
    plt.bar(range(gems_count), gems[gems_count:], label = 'Train data')
    plt.bar(range(gems_count), gems[0:gems_count], label = 'Test data')
else:
    plt.bar(range(gems_count), gems[0:gems_count], label = 'Train data')
    plt.bar(range(gems_count), gems[gems_count:], label = 'Test data')
ax.grid()
ax.legend(fontsize = 12);

from google.colab import drive
drive.mount('/content/drive')

import os
import matplotlib.pyplot as plt
import seaborn as sn
import tensorflow as tf

import cv2
from random import randint

import numpy as np
# from tensorflow.keras.optimizers import Adam
import tensorflow as tf 
from tensorflow import keras

img_w, img_h = 128, 128    # width and height of image
hist_w, hist_h = 192, 192    # width and height of image
# train_dir = '/kaggle/input/gemstones-images/train/'
train_dir = "/content/drive/MyDrive/FYP Implementation/DataSet-01/train"

def read_imgs_lbls(_dir):
    Images, Labels, Hists, Masks, Results = [], [], [], [], []
    for root, dirs, files in os.walk(_dir):
        f = os.path.basename(root)  # get class name - Amethyst, Onyx, etc       
        for file in files:
            Labels.append(f)
            try:
                image = cv2.imread(root+'/'+file)
                
                hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
                original = image.copy()
                hsv_lower = np.array([0,0,20])
                hsv_upper = np.array([255,255,255])
                mask = cv2.inRange(hsv, hsv_lower, hsv_upper)
                result = cv2.bitwise_and(original, original, mask=mask)
                
                image = cv2.resize(image,(int(img_w*1.5), int(img_h*1.5)))       # resize the image (images are different sizes)
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB);
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converts an image from BGR color space to RGB
                image_rgb[:,:,0] = cv2.equalizeHist(image_rgb[:,:,0])
                image_output = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)

                Images.append(image)
                Hists.append(image_output)
                Masks.append(mask)
                Results.append(result)
            except Exception as e:
                print(e)
    Images = np.array(Images)
    Hists = np.array(Hists)
    Masks = np.array(Masks)
    Results = np.array(Results)
    return (Images, Labels, Hists, Masks, Results)

def get_class_index(Labels):
    for i, n in enumerate(Labels):
        for j, k in enumerate(CLASSES):    # foreach CLASSES
            if n == k:
                Labels[i] = j
    Labels = np.array(Labels)
    return Labels

Train_Imgs, Train_Lbls, Train_Hists, Train_Masks, Train_Results = read_imgs_lbls(train_dir)
Train_Lbls = get_class_index(Train_Lbls)
print('Shape of train images: {}'.format(Train_Imgs.shape))
print('Shape of train histograms: {}'.format(Train_Hists.shape))
print('Shape of train labels: {}'.format(Train_Lbls.shape))

dim = 4 #you can change it;  4x4 dimension flat plot

f,ax = plt.subplots(dim,dim) 
f.subplots_adjust(0,0,2,2)
for i in range(0,dim):
    for j in range(0,dim):
        rnd_number = randint(0,len(Train_Imgs)-1)
        cl = Train_Lbls[rnd_number]
        ax[i,j].imshow(Train_Imgs[rnd_number])
        ax[i,j].set_title(CLASSES[cl]+': ' + str(cl))
        ax[i,j].axis('off')

high_thresh, low_thresh = 60, 40
def edge_and_cut(img):
    try:
        img = cv2.GaussianBlur(img, (7, 7), 0)
        edges = cv2.Canny(img, high_thresh, low_thresh)
        #img = remove_shadows(img)
        
        if(np.count_nonzero(edges)>edges.size/10000):           
            pts = np.argwhere(edges>0)
            y1,x1 = pts.min(axis=0)
            y2,x2 = pts.max(axis=0)
            
            new_img = img[y1:y2, x1:x2]           # crop the region
            new_img = cv2.resize(new_img,(img_w, img_h))  # Convert back
        else:
            new_img = cv2.resize(img,(img_w, img_h))
    
    except Exception as e:
        print(e)
        new_img = cv2.resize(img,(img_w, img_h))
    
    return new_img

def remove_background(img):
    rgb_planes = cv2.split(img)

    result_planes = []
    result_norm_planes = []
    for plane in rgb_planes:
        dilated_img = cv2.dilate(plane, np.ones((3,3), np.uint8), iterations=1)
        bg_img = cv2.medianBlur(dilated_img, 21)
        diff_img = 255 - cv2.absdiff(plane, bg_img)
        norm_img = cv2.normalize(diff_img,None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)
        result_planes.append(diff_img)
        result_norm_planes.append(norm_img)

    result = cv2.merge(result_planes)
    result_norm = cv2.merge(result_norm_planes)
    
    return result

def show_cropped(img):
    emb_img = img.copy()
    high_thresh, low_thresh = 60, 40
    edges = cv2.Canny(img, high_thresh, low_thresh)
    #img = remove_background(img)
    
    if(np.count_nonzero(edges)>edges.size/10000):
        pts = np.argwhere(edges>0)
        y1,x1 = pts.min(axis=0)
        y2,x2 = pts.max(axis=0)

        new_img = img[y1:y2, x1:x2]  

        edge_size = 2 #replace it with bigger size for larger images            

        emb_img[y1-edge_size:y1+edge_size, x1:x2] = [255, 0, 0]
        emb_img[y2-edge_size:y2+edge_size, x1:x2] = [255, 0, 0]
        emb_img[y1:y2, x1-edge_size:x1+edge_size] = [255, 0, 0]
        emb_img[y1:y2, x2-edge_size:x2+edge_size] = [255, 0, 0]

        new_img = cv2.resize(new_img,(img_w, img_h))  # Convert to primary size  
        
    else:
        new_img = cv2.resize(img,(img_w, img_h))
    
    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))
    ax[0].imshow(img, cmap='gray')
    ax[0].set_title('Original Image', fontsize=14)
    ax[1].imshow(edges, cmap='gray')
    ax[1].set_title('Canny Edges', fontsize=14)
    ax[2].imshow(emb_img, cmap='gray')
    ax[2].set_title('Bounding Box', fontsize=14)       
    ax[3].imshow(new_img, cmap='gray')
    ax[3].set_title('Cropped', fontsize=14)

for x in range(0,3):
    show_cropped(Train_Results[randint(0,len(Train_Imgs))])

def crop_images(Imgs):
    CroppedImages = np.ndarray(shape=(len(Imgs), img_w, img_h, 3), dtype=np.int)

    ind = 0
    for im in Imgs: 
        x = edge_and_cut(im)
        CroppedImages[ind] = x
        ind += 1

    return CroppedImages

Train_Imgs = crop_images(Train_Imgs)
Train_Results = crop_images(Train_Results)
print('Final shape of images in train set: {} '.format(Train_Imgs.shape))
print('Final shape of images in train set: {} '.format(Train_Results.shape))

from sklearn.model_selection import train_test_split
X1_train, X1_val, y1_train, y1_val = train_test_split(Train_Results, Train_Lbls, shuffle = True, test_size = 0.2, random_state = 42)
print('Shape of X_train: {}, y_train: {} '.format(X1_train.shape, y1_train.shape))
print('Shape of X_val: {}, y_val: {} '.format(X1_val.shape, y1_val.shape))

from tensorflow.python.client import device_lib
devices = device_lib.list_local_devices()
print(devices)

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import optimizers

filters = 32      # the dimensionality of the output space
kernel_size = 3   # length of the 2D convolution window
max_pool = 2      # size of the max pooling windows

EPOCHS = 40                                  # while testing you can change it
batch_size = 64 # number of training samples using in each mini batch during GD (gradient descent) 
iter_per_epoch1 = len(X1_train) // batch_size  # each sample will be passed [iter_per_epoch] times during training
val_per_epoch1 = len(X1_val) // batch_size     # each sample will be passed [val_per_epoch] times during validation

# iter_per_epoch2 = len(X2_train) // batch_size  # each sample will be passed [iter_per_epoch] times during training
# val_per_epoch2 = len(X2_val) // batch_size     # each sample will be passed [val_per_epoch] times during validation
print(iter_per_epoch1)

model1 = Sequential()

# first layer
model1.add(Conv2D(batch_size, (kernel_size, kernel_size), activation='relu', padding='same', input_shape=(img_w, img_h, 3))) # 32
model1.add(MaxPooling2D((max_pool, max_pool))) #reduce the spatial size of incoming features

# second layer
model1.add(Conv2D(2*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 64
model1.add(MaxPooling2D((max_pool, max_pool))) 
model1.add(Dropout(0.2))

# third layer
model1.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128
model1.add(MaxPooling2D((max_pool, max_pool))) 
model1.add(Dropout(0.2))

# fourth layer
model1.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128
model1.add(AveragePooling2D(pool_size= (2, 2), strides= (2, 2))) 
model1.add(Dropout(0.2))

# fifth layer
model1.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128
model1.add(MaxPooling2D((max_pool, max_pool))) 
model1.add(Dropout(0.2))

model1.add(Flatten())
model1.add(Dense(4*batch_size, activation='relu'))                                             # 512
model1.add(Dense(87, activation='softmax'))
model1.summary()

model1.compile(optimizer=tf.optimizers.Adam(learning_rate=.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
# model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

from keras.preprocessing.image import ImageDataGenerator

# TODO: I think I'll want to change here
train_datagen = ImageDataGenerator(              # this is the augmentation configuration used for training
        rotation_range=25,
        zoom_range=0.1,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.2,
        horizontal_flip=True,
        vertical_flip=True,
        channel_shift_range=0.5,
        )

val_datagen = ImageDataGenerator()

n = randint(0,len(X1_train))
samples = np.expand_dims(X1_train[n], 0)
it = train_datagen.flow(samples, batch_size=batch_size)
cols = 7

fig, ax = plt.subplots(nrows=1, ncols=cols, figsize=(15, 10))
ax[0].imshow(X1_train[n], cmap='gray')
ax[0].set_title('Original', fontsize=10)

for i in range(1,cols):
    batch = it.next()    # generate batch of images 
    image = batch[0].astype('uint32') # convert to unsigned int for viewing
    ax[i].set_title('augmented {}'.format(i), fontsize=10)
    ax[i].imshow(image, cmap='gray')

train_gen1 = train_datagen.flow(X1_train, y1_train, batch_size=batch_size)
val_gen1 = val_datagen.flow(X1_val, y1_val, batch_size=batch_size)

m1 = model1.fit_generator(
       train_gen1,
       steps_per_epoch= iter_per_epoch1,
       epochs=EPOCHS, 
       validation_data = val_gen1,
       validation_steps = val_per_epoch1,
       verbose = 1 # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.
       )

fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,5))
axs[0].plot(m1.history['accuracy'])
axs[0].plot(m1.history['val_accuracy'])
axs[0].set_title('Model1 accuracy')
axs[0].legend(['Train', 'Val'], loc='upper left')

axs[1].plot(m1.history['loss'])
axs[1].plot(m1.history['val_loss'])
axs[1].set_title('Model1 loss')
axs[1].legend(['Train', 'Val'], loc='upper left')

for ax in axs.flat:
    ax.set(xlabel='Epoch')

score = model1.evaluate_generator(val_gen1, steps= len(val_gen1))

for idx, metric in enumerate(model1.metrics_names):
    print('{}:{}'.format(metric, score[idx]))

from sklearn.metrics import confusion_matrix
y_pre_test=model1.predict(X1_val)
y_pre_test=np.argmax(y_pre_test,axis=1)
cm=confusion_matrix(y1_val,y_pre_test)

plt.figure(figsize = (15,15))
sn.heatmap(cm, annot=True)

x=(y_pre_test-y1_val!=0).tolist()
x=[i for i,l in enumerate(x) if l!=False]

fig,ax=plt.subplots(1,5,sharey=False,figsize=(13,13))
fig.tight_layout()

for i in range(5):
    ax[i].imshow(X1_val[x[i]][:,:,1])
    ax[i].set_xlabel('{}, Pred: {}'.format(CLASSES[y1_val[x[i]]],CLASSES[y_pre_test[x[i]]]))

red_stones = ['Almandine', 'Garnet Red', 'Hessonite', 'Pyrope', 'Rhodolite']
red_stones = get_class_index(red_stones)

fig,ax=plt.subplots(1,len(red_stones),sharey=False,figsize=(13,13))
fig.tight_layout()

for i in range(len(red_stones)):
    ax[i].imshow(Train_Imgs[np.where(Train_Lbls==red_stones[i])[0][1]])
    ax[i].set_xlabel(CLASSES[red_stones[i]])

model1.save('model_gemstones.h5')

test_dir = '/content/drive/MyDrive/FYP Implementation/DataSet-01/test'

Test_Imgs, Test_Lbls, Test_Hists, Test_Masks, Test_Results = read_imgs_lbls(test_dir)
Test_Lbls = get_class_index(Test_Lbls)

Test_Results = crop_images(Test_Results)
print('shape of images in test set: {} '.format(Test_Results.shape))

# f,ax = plt.subplots(5,5) 
# f.subplots_adjust(0,0,2,2)
# for i in range(0,5,1):
#     for j in range(0,5,1):
#         rnd_number = randint(0,len(Test_Results))
#         pred_image = np.array([Test_Results[rnd_number]])
#         pred_class =model1.predict_classes(pred_image)[0] #(model1.predict(pred_image) > 0.5).astype("int32") 
#         pred_prob = model1.predict(pred_image).reshape(87)
#         act = CLASSES[Test_Lbls[rnd_number]]
#         ax[i,j].imshow(Test_Results[rnd_number])
#         ax[i,j].imshow(pred_image[0])
#         if(CLASSES[pred_class] != CLASSES[Test_Lbls[rnd_number]]):
#             t = '{} [{}]'.format(CLASSES[pred_class], CLASSES[Test_Lbls[rnd_number]])
#             ax[i,j].set_title(t, fontdict={'color': 'darkred'})
#         else:
#             t = '[OK] {}'.format(CLASSES[pred_class]) 
#             ax[i,j].set_title(t)
#         ax[i,j].axis('off')

newimage="/content/drive/MyDrive/FYP Implementation/DataSet-01/test/Andradite/andradite_18.jpg"
new_image=cv2.imread(newimage)
image=cv2.resize(new_image,(128,128))
image=cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

image=tf.image.resize(image,size=[128,128])
image=tf.reshape(image,[1,128,128,3])
predict_x=model1.predict(image) 

classes_x=np.argmax(predict_x,axis=1)

print(CLASSES[classes_x[0]])
print(classes_x)
print(CLASSES)

model = keras.models.load_model('/content/model_gemstones.h5')


newimage="/content/drive/MyDrive/FYP Implementation/DataSet-01/test/Amazonite/amazonite_18.jpg"
new_image=cv2.imread(newimage)

image=cv2.resize(new_image,(128,128))
image=cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
image=tf.image.resize(image,size=[128,128])
image=tf.reshape(image,[1,128,128,3])
predict_x=model.predict(image) 
classes_x=np.argmax(predict_x,axis=1)

print(CLASSES[classes_x[0]])
print(classes_x)
print(CLASSES)